programmable L1 cache/Scratchpad memory
Accessible only in a thread block
Useful for repeated small data or corrdination
```
__shared__ float a[N]
__shared__ unsigned s;

a[id] = id;
if (id == 0) s = 1;
```

Example
```
#include <stdio.h>
#include <cude.h>

#define BLOCKSIZE 1024

__global__ void dkernel(){
    __shared__ unsigned s;
    if(threadIdx.x == 0) s = 0;
    if(threadIdx.x == 1) s += 1;
    if(threadIdx.x == 100) s += 2;
    if(threadIdx.x == 0) printf("s%d\n",s);

}
int main(){
    dkernel <<<2,BLOCKSIZE>>>();
    cudaDeviceSynchronize();
}

```
the output is:
s=3
s=3
s=3
s=3
s=3
s=3
s=3
s=3
s=1
s=3
s=3
s=3
s=3
s=3
s=3
s=3
s=3
s=3

The variable s is shared among all threads in a block. You’re setting s to 0 if threadIdx.x == 0, adding 1 if threadIdx.x == 1, and adding 2 if threadIdx.x == 100. Then, you’re printing s if threadIdx.x == 0.

The output s=3 suggests that the threads with threadIdx.x equal to 0, 1, and 100 have all executed before the print statement. The output s=1 suggests that only the threads with threadIdx.x equal to 0 and 1 have executed before the print statement.

The variation in output is due to the fact that the execution order of threads in a block is not guaranteed in CUDA. This is why it’s generally not safe to have threads in a block depend on the computation of other threads in the same block without proper synchronization. In your case, you might want to consider using __syncthreads() to synchronize the threads in a block. This will ensure that all threads in a block have reached the same point in the code before proceeding, which can help avoid race conditions. Here’s how you could modify the code:
```
__global__ void dkernel(){
    __shared__ unsigned s;
    if(threadIdx.x == 0) s = 0;
    __syncthreads();
    if(threadIdx.x == 1) s += 1;
    __syncthreads();
    if(threadIdx.x == 100) s += 2;
    __syncthreads();
    if(threadIdx.x == 0) printf("s%d\n",s);
}

```