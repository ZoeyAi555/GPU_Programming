- concurrent computation using streams
- overlapped computation and communication
- cooperative kernels
- callbacks
- events

## Asynchronous processing
- independent tasks run simultaneously
- computation on the host / device
    - kernel launches
    - memcpy within a device
    - less than 64 KB memcpy from CPU to GPU
    - memcpy by async functions
    - memset function calls
    - kernel use large local mem is less likely to concurrently with other kernels
- CPU to GPU memory transfer
- GPU to CPU memory transfer
- GPU to GPU memory transfer on a device/ accross devices
    - all above(CPU GPU)
    - overlap cpoies and from device
    - compute capability large than 2.x

```
__global__ void K1() {
    unsigned num = 0;
    for (unsigned ii = 0; ii < threadldx.x; ++ii)
        num += ii;
    printf("K1: %d\n", threadldx.x);
}
__global__ void K2() { 
    printf("K2\n"); 
}
int main() {
    cudaStream_t s1, s2; 
    cudaStreamCreate(&s1); 
    cudaStreamCreate(&s2);
    K1<<<1,1024,0 s1>>>(); 
    K2<<<1, 32, 0, s2>>>();// need stream to make them concurrent
    cudaDeviceSynchronize(); 
    return 0;
}

```

### Streams
- A stream is a queue of device work
    - Host enqueues work and returns immediately.
    - Device schedules work from streams when resources are free
- CUDA operations are placed within a stream 
    - kernel launches, memcpy, etc.
- Operations within a stream are ordered and cannot overlap.
- Operations across streams are unordered and can overlap.	